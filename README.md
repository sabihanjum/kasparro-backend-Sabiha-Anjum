# Kasparro Backend - Production Data Ingestion System

A production-grade backend system for ingesting data from multiple sources (APIs, CSV files) with incremental processing, comprehensive API endpoints, and cloud deployment support.

## ğŸ¯ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  External Data Sources                   â”‚
â”‚        (APIs, CSV files, RSS feeds, databases)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   ETL Service  â”‚
         â”‚  (Ingestion)   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Raw    â”‚      â”‚Normalizedâ”‚
    â”‚ Tables â”‚â”€â”€â”€â”€â”€â”€â”‚ Tables   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  PostgreSQL â”‚
          â”‚  Database   â”‚
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ FastAPI App â”‚
          â”‚ (GET/POST)  â”‚
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   Clients   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ… Tier Completion Status

### P0 - Foundation (REQUIRED) âœ“
- [x] P0.1 - Data Ingestion (API + CSV sources)
- [x] P0.2 - Backend API Service (/data, /health)
- [x] P0.3 - Dockerized System (Docker, compose, Makefile)
- [x] P0.4 - Basic Test Suite

### P1 - Growth Layer (REQUIRED) ğŸš€
- [ ] P1.1 - Third Data Source (RSS/API)
- [ ] P1.2 - Checkpoint & Recovery Logic
- [ ] P1.3 - /stats Endpoint
- [ ] P1.4 - Comprehensive Tests
- [ ] P1.5 - Clean Architecture (âœ“ Already implemented)

### P2 - Differentiator (OPTIONAL) ğŸŒŸ
- [ ] P2.1 - Schema Drift Detection
- [ ] P2.2 - Failure Injection + Recovery
- [ ] P2.3 - Rate Limiting + Backoff
- [ ] P2.4 - Observability Layer
- [ ] P2.5 - DevOps Enhancements
- [ ] P2.6 - Run Comparison / Anomaly Detection

## ğŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose
- Python 3.11+ (for local development)
- PostgreSQL 15+ (if running without Docker)

### Running with Docker (Recommended)

```bash
# Clone repository
git clone https://github.com/yourusername/kasparro-backend-yourname
cd kasparro-backend-yourname

# Start services
make up

# Services will be available at:
# - Backend API: http://localhost:8000
# - API Docs: http://localhost:8000/docs
# - Database: localhost:5432
```

### Running Locally

```bash
# Install dependencies
pip install -r requirements.txt

# Setup environment
cp .env.example .env
# Edit .env and add your API_KEY

# Start PostgreSQL (ensure it's running)
# Then run the application
python -m uvicorn src.api.main:app --reload

# Run tests
make test
```

## ğŸ“Š Available Endpoints

### Health & Status
```
GET /health
Returns: Database connectivity and last ETL run status

GET /stats
Returns: ETL statistics, recent runs, processing metadata
```

### Data Access
```
GET /data?limit=10&offset=0&source=optional_source
Query Parameters:
  - limit (1-100): Number of records
  - offset (0+): Pagination offset
  - source: Filter by source (optional)

Returns: Paginated results with request_id and api_latency_ms
```

### Interactive API Documentation
```
GET /docs    # Swagger UI
GET /redoc   # ReDoc
```

## ğŸ”§ Configuration

### Environment Variables (.env)
```
# Database
DATABASE_URL=postgresql://user:password@localhost:5432/kasparro

# API Authentication
API_KEY=your-api-key-here

# ETL Configuration
ETL_BATCH_SIZE=100
ETL_CHECKPOINT_INTERVAL=100

# Application
ENVIRONMENT=production
DEBUG=false
LOG_LEVEL=INFO
```

## ğŸ—„ï¸ Database Schema

### Raw Data Tables
- `raw_data_api` - Raw ingested API records
- `raw_data_csv` - Raw ingested CSV records

### Processed Data Tables
- `normalized_data` - Unified schema across all sources
- `etl_checkpoint` - Tracks ingestion progress (incremental)
- `etl_run` - Metadata for each ETL execution

## ğŸ“ Project Structure

```
kasparro-backend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ main.py          # FastAPI application & endpoints
â”‚   â”‚   â””â”€â”€ routes.py        # Route definitions
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py        # Configuration management
â”‚   â”‚   â”œâ”€â”€ database.py      # Database setup
â”‚   â”‚   â”œâ”€â”€ logging_config.py# Logging configuration
â”‚   â”‚   â””â”€â”€ models.py        # SQLAlchemy ORM models
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ runner.py        # ETL orchestration
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ ingestion.py     # Data ingestion service
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ data.py          # Pydantic validation schemas
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_api.py      # API endpoint tests
â”‚   â”‚   â””â”€â”€ test_ingestion.py# ETL service tests
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â””â”€â”€ test_e2e.py      # End-to-end tests
â”‚   â””â”€â”€ conftest.py          # Test fixtures
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample.csv           # Sample CSV for testing
â”œâ”€â”€ Dockerfile               # Docker image definition
â”œâ”€â”€ docker-compose.yml       # Multi-container setup
â”œâ”€â”€ Makefile                 # Development commands
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ .env.example             # Environment template
â””â”€â”€ README.md               # This file
```

## ğŸ§ª Testing

### Run All Tests
```bash
make test
```

### Run Unit Tests Only
```bash
make test-unit
```

### Run Integration Tests
```bash
make test-integration
```

### Test Coverage
```bash
pytest tests/ --cov=src --cov-report=html
open htmlcov/index.html
```

## ğŸ”„ ETL Pipeline

### Ingestion Flow
1. **Fetch** - Get data from API/CSV sources
2. **Store** - Save raw data to `raw_data_*` tables
3. **Normalize** - Transform to unified schema
4. **Checkpoint** - Track processing progress
5. **Validate** - Type checking with Pydantic

### Key Features
- **Incremental Processing**: Only processes new records via checkpoints
- **Idempotent Writes**: Prevents duplicates
- **Error Recovery**: Resumes from last checkpoint on failure
- **Batch Operations**: Commits in configurable batches
- **Secure Authentication**: Handles API keys securely (via env vars)

### Running ETL Manually
```python
import asyncio
from src.ingestion.runner import run_etl_with_backoff

sources = {
    "api_source": {
        "type": "api",
        "url": "https://api.example.com/data",
    },
    "csv_source": {
        "type": "csv",
        "path": "data/sample.csv",
    },
}

result = asyncio.run(run_etl_with_backoff(sources))
print(result)
```

## ğŸ› ï¸ Makefile Commands

```bash
make up               # Start all containers
make down             # Stop containers
make restart          # Restart containers
make logs             # View backend logs
make logs-db          # View database logs
make test             # Run all tests with coverage
make test-unit        # Run unit tests only
make test-integration # Run integration tests
make clean            # Clean up containers, volumes
make migrate          # Run database migrations
make shell            # Access backend container shell
make psql             # Connect to PostgreSQL
make lint             # Check code quality
make format           # Format code (black, isort)
make type-check       # Type checking with mypy
```

## ğŸ“ˆ Monitoring & Logging

### Logs
- Console output (structured JSON in production)
- Database query logs (in debug mode)
- ETL execution logs with timestamps

### Metrics Available via /stats
- Total records processed
- Records inserted/updated
- Processing failures
- Last run status and duration
- Historical run data

## ğŸš€ Cloud Deployment

### AWS Deployment
```bash
# Push to ECR
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin <account>.dkr.ecr.us-east-1.amazonaws.com
docker tag kasparro-backend:latest <account>.dkr.ecr.us-east-1.amazonaws.com/kasparro:latest
docker push <account>.dkr.ecr.us-east-1.amazonaws.com/kasparro:latest

# Deploy with ECS/Fargate
# (See AWS documentation for detailed steps)

# Schedule ETL with EventBridge
# (Create cron rule pointing to Lambda/ECS task)
```

### GCP Deployment
```bash
# Push to Artifact Registry
gcloud auth configure-docker us-central1-docker.pkg.dev
docker tag kasparro-backend:latest us-central1-docker.pkg.dev/PROJECT_ID/repo/kasparro:latest
docker push us-central1-docker.pkg.dev/PROJECT_ID/repo/kasparro:latest

# Deploy with Cloud Run
gcloud run deploy kasparro-backend \
  --image us-central1-docker.pkg.dev/PROJECT_ID/repo/kasparro:latest \
  --platform managed
```

## ğŸ” Security Best Practices

âœ… **Implemented**
- API keys loaded from environment variables (never hardcoded)
- Non-root user in Docker container
- Health checks for container orchestration
- Connection pooling for database safety
- SQL injection prevention via ORM

âœ… **Recommended for Production**
- Use secret management (AWS Secrets Manager, GCP Secret Manager)
- Enable HTTPS/TLS for API
- Implement API rate limiting & authentication
- Enable database encryption at rest
- Set up VPC endpoints for private connectivity
- Regular security audits and dependency updates

## ğŸ› Troubleshooting

### Database Connection Error
```bash
# Check database is running
make logs-db

# Reset database
docker-compose down -v
make up
```

### API Won't Start
```bash
# Check logs
make logs

# Verify environment variables
cat .env

# Test connection
make psql
```

### Tests Failing
```bash
# Install test dependencies
pip install pytest pytest-asyncio pytest-cov

# Run with verbose output
pytest tests/ -vv
```

## ğŸ“š API Examples

### Get All Data (with Pagination)
```bash
curl "http://localhost:8000/data?limit=20&offset=0"
```

### Filter by Source
```bash
curl "http://localhost:8000/data?source=api_source&limit=10"
```

### Health Check
```bash
curl http://localhost:8000/health
```

### View Statistics
```bash
curl "http://localhost:8000/stats?limit=5"
```

## ğŸ“ Next Steps (P1/P2)

1. **Add Third Data Source** (P1.1)
   - Implement RSS feed ingestion
   - Add another API endpoint source
   - Unify schema across 3+ sources

2. **Enhance Recovery** (P1.2)
   - Implement failure injection testing
   - Add detailed error logging
   - Build recovery dashboard

3. **Add Observability** (P2.4)
   - Prometheus metrics endpoint
   - Distributed tracing with OpenTelemetry
   - Dashboard with Grafana

4. **Production Hardening** (P2.5)
   - GitHub Actions CI/CD
   - Automated image publishing
   - Deployment automation

## ğŸ“§ Support & Questions

For issues, questions, or suggestions:
- Open an issue on GitHub
- Check existing documentation
- Review test files for usage examples

## ğŸ“„ License

This project is provided as-is for the Kasparro hiring process.

---

**Last Updated**: December 2024  
**Status**: Production-Ready (P0 + P1 Core)
